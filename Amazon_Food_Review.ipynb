{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon Food Review.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmxtK3HR3nDWfydDSRXM83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neetika23/Machine-Learning/blob/master/Amazon_Food_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9iyaTGBpGB2",
        "colab_type": "text"
      },
      "source": [
        "##Amazon Fine Food review Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO4Zbkbuf64N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c02ff6b8-ede1-447d-d1a9-993be56a48e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RlzGvJJpMWX",
        "colab_type": "text"
      },
      "source": [
        "**Objective:** Given a review, tell whether a review is positive or not? This can be done with the help of score/rating. But we will not keep this score column and will predict the review without using the score column, because if we use score, it can be done easily with if-else statement. ***Text*** is the most important piece of information here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNuGV2WIosuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "afdffe43-f7cf-4f4a-91bf-080d699e322f"
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhXv-t0ozolN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Opens connection to database\n",
        "con = sqlite3.connect('/content/drive/My Drive/amazon_fine_food/database.sqlite')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R9YUtIX-F_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filtering only positive and negative review i.e not taking into consideration the review\n",
        "# which are neutral, score=3.\n",
        "filter_data = pd.read_sql_query(\"\"\"\n",
        "SELECT *\n",
        "FROM Reviews\n",
        "WHERE Score != 3\n",
        "\"\"\", con)\n",
        "# Run this sql command using con (connection)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3o0VjwBBQIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give score > 3 as positive and score < 3 as negative\n",
        "# Replacing the values of score col as positive or negative.\n",
        "def partition(x):\n",
        "  if x < 3:\n",
        "    return 'negative'\n",
        "  return 'positive'\n",
        "\n",
        "# Changing reviews with score to be positive and negative.\n",
        "actualScore = filter_data['Score']\n",
        "positiveNegative = actualScore.map(partition) # Using function to replace data of score column\n",
        "filter_data['Score'] = positiveNegative"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXzjZYB8By6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b8339847-e3a0-4744-c525-09d45d518d60"
      },
      "source": [
        "filter_data.shape\n",
        "filter_data.head()\n",
        "\n",
        "# Time is stored as a unix time stamp."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>negative</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>positive</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPnpZbOZQiA0",
        "colab_type": "text"
      },
      "source": [
        "##Data Cleaning: Deduplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE-HP1TcQnQg",
        "colab_type": "text"
      },
      "source": [
        "The reviews data has many duplicate entries. Hence it was necessary to remove them in order to get unbiased results for the analysis of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35gYofYEQ5OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "266539c9-2b8c-4664-e2fd-c4ae0bad31b5"
      },
      "source": [
        "# In the below query we sort the reviews given by the user of that particular user id, sorting is done based on productid.\n",
        "display = pd.read_sql_query(\"\"\"\n",
        "SELECT * \n",
        "FROM Reviews\n",
        "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
        "ORDER BY ProductId\n",
        "\"\"\",con)\n",
        "display\n",
        "\n",
        "# It can be seen that except product id all the data is same. How can a user reviews more than one product at a particular time stamp.\n",
        "# To look for a product in amazon with a particular product id, use amazon.com/dp/<product_id> (ASIN- Amazon Standard Identification No.)\n",
        "# The things was that it was a product with different flavours, so the user gave a common review and the it got stored with different\n",
        "# product id.\n",
        "# Product review got shared for other as the other was very similar to the particular product.\n",
        "# These data does not add value to dataset."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78445</td>\n",
              "      <td>B000HDL1RQ</td>\n",
              "      <td>AR5J8UI46CURR</td>\n",
              "      <td>Geetha Krishnan</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1199577600</td>\n",
              "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
              "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>138317</td>\n",
              "      <td>B000HDOPYC</td>\n",
              "      <td>AR5J8UI46CURR</td>\n",
              "      <td>Geetha Krishnan</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1199577600</td>\n",
              "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
              "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>138277</td>\n",
              "      <td>B000HDOPYM</td>\n",
              "      <td>AR5J8UI46CURR</td>\n",
              "      <td>Geetha Krishnan</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1199577600</td>\n",
              "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
              "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>73791</td>\n",
              "      <td>B000HDOPZG</td>\n",
              "      <td>AR5J8UI46CURR</td>\n",
              "      <td>Geetha Krishnan</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1199577600</td>\n",
              "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
              "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>155049</td>\n",
              "      <td>B000PAQ75C</td>\n",
              "      <td>AR5J8UI46CURR</td>\n",
              "      <td>Geetha Krishnan</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1199577600</td>\n",
              "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
              "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Id  ...                                               Text\n",
              "0   78445  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
              "1  138317  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
              "2  138277  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
              "3   73791  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
              "4  155049  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRxodQLySbnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets remove them.\n",
        "# Sorting the data according to the product_id in ascending order.\n",
        "sort_data = filter_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9b9Be8tUhKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9adc08f-5236-447c-fa8b-9a11096fc215"
      },
      "source": [
        "# Duplicates can be removed if something is common, so in this case, a data will be considered as the duplicate,\n",
        "# when userid,profilename,time and text will be same. We will keep the first data point (productid) and delete the rest of the duplicate data points.\n",
        "# inplace = false means it will return a (copy) value for you, instead to drop duplicates in place.\n",
        "final = sort_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first', inplace=False)\n",
        "final.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364173, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f342Z9_V2KS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebe78202-5e07-4a48-f3eb-b3e07cbe0e3c"
      },
      "source": [
        "# Checking how much % of data still remains after removing duplicates\n",
        "(final['Id'].size*1.0/filter_data['Id'].size*1.0)*100\n",
        "\n",
        "#69% of data remaining after this cleaning."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.25890143662969"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhAFe3QdWDoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "33f56068-c0c9-44db-c769-9710c43587b6"
      },
      "source": [
        "# Now there is another problem with data.\n",
        "# Helpfulness Numerator which is how many people said Yes to the review should be less than Helpfullness Denominator, which is\n",
        "# how many people said Yes + how many people said No.\n",
        "# But we find that in some data points HN > HD, which is an error, so this needs to be removed.\n",
        "display = pd.read_sql_query(\"\"\"\n",
        "SELECT *\n",
        "FROM Reviews\n",
        "WHERE Score != 3 AND Id = 44737 OR Id = 64422\n",
        "ORDER BY ProductId\n",
        "\"\"\",con)\n",
        "display"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64422</td>\n",
              "      <td>B000MIDROQ</td>\n",
              "      <td>A161DK06JJMCYF</td>\n",
              "      <td>J. E. Stephens \"Jeanne\"</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1224892800</td>\n",
              "      <td>Bought This for My Son at College</td>\n",
              "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44737</td>\n",
              "      <td>B001EQ55RW</td>\n",
              "      <td>A2V0I904FH7ABY</td>\n",
              "      <td>Ram</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1212883200</td>\n",
              "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
              "      <td>It was almost a 'love at first bite' - the per...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id  ...                                               Text\n",
              "0  64422  ...  My son loves spaghetti so I didn't hesitate or...\n",
              "1  44737  ...  It was almost a 'love at first bite' - the per...\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxWQM3wlXXA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# So keep those data points where HN <= HD\n",
        "final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpbOi2jDXlj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "848a598f-d30b-4dd0-da95-ac9ea9ee9203"
      },
      "source": [
        "print(final.shape)\n",
        "\n",
        "#How many positive and negative reviews are there in pur dataset?\n",
        "final['Score'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(364171, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    307061\n",
              "negative     57110\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXbQmFgkDYOT",
        "colab_type": "text"
      },
      "source": [
        "##Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ka_d6TXDbiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BoW\n",
        "count_vec = CountVectorizer()\n",
        "# We have count Vectorizer in sckit learn\n",
        "\n",
        "# To compute BoW we just have to do as follows\n",
        "final_counts = count_vec.fit_transform(final['Text'].values)\n",
        "# Get the text col from final df and convert them into values. "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9El9vtKEqvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4befc92b-1faf-4bb5-b300-934d7fcbc28c"
      },
      "source": [
        "type(final_counts)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgqYgIKFE6wL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6761eb4-667d-4f9b-eaf4-14d2f36f261f"
      },
      "source": [
        "final_counts.get_shape()\n",
        "# We get matrix with the printed rows and columns.\n",
        "# Each row corresponds to the review and each col (dimension) corresponds to the unique words.\n",
        "# Dimensionality of this vector is 115281."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364171, 115281)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRqm8PVBjIXu",
        "colab_type": "text"
      },
      "source": [
        "##Text-preprocessing\n",
        "1. Remove html tags.\n",
        "2. Remove punctuation marks.\n",
        "3. Check for alpha-numeric.\n",
        "4. Check if len > 2.\n",
        "5. Convert to lowercase.\n",
        "6. Remove stopwords.\n",
        "7. Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ToLGbFcjqGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "617bc617-3ed9-4e46-cbea-e25c1c34b367"
      },
      "source": [
        "# find sentences containing html tags\n",
        "import re # regular expression\n",
        "i=0;\n",
        "for sent in final['Text'].values:\n",
        "  if(len(re.findall('<.*?>',sent))):\n",
        "    print(i)\n",
        "    print(sent)\n",
        "    break;\n",
        "  i += 1;\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcbzSFMJoAs-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a189c616-1401-4359-fc6d-2647fcad1d38"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYPrUlrUkhJs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0437a92c-4a76-4f13-f886-118daa1690c9"
      },
      "source": [
        "# Link: https://pymotw.com/2/re/\n",
        "# nltk : natural lang processing toolkit\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop = set(stopwords.words('english')) # set of all stopwords stored in stop\n",
        "# Initializing stemmer, also specifying the lang as english because it can also be done for non-english lang.\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "# Given any sentence, it will find all the text in < > and replace this it one space.\n",
        "def cleanhtml(sentence):\n",
        "  cleanr = re.compile('<.*?>')  # find any text included in < >, here .*? means any text.(check out documentation)\n",
        "  cleantext = re.sub(cleanr, ' ', sentence) # sub: substitute , here means that substitute with space\n",
        "  return cleantext\n",
        "\n",
        "def cleanpunc(sentence):\n",
        "  cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence) # | means logical OR, we replace ?!\\'\"3 with space\n",
        "  cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "  return cleaned\n",
        "\n",
        "print(stop) # see list of stopwords\n",
        "print('***********************************************************')\n",
        "print(sno.stem('tasty')) # find the stem word of tasty. Its \"tasti\"."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ain', 'her', \"wouldn't\", 'were', \"you're\", 's', 'doesn', 'myself', 'too', 'was', 'themselves', 'our', \"wasn't\", 'yourselves', 'not', 'being', \"should've\", 'out', \"doesn't\", 'isn', \"you'll\", 'during', \"shan't\", 'nor', 'with', 'both', 'ourselves', 've', 'wouldn', 'hers', 'such', 'an', 'they', 'then', 'in', 'just', 'while', 'into', 'why', \"you'd\", \"aren't\", 'y', 'ma', 'yourself', 'through', 'other', 'mustn', 'you', 'me', 'what', \"she's\", 'above', 'will', 'don', \"haven't\", 'down', \"you've\", \"won't\", 'are', 'after', 'wasn', 'as', 'about', 'ours', 'who', 'very', \"weren't\", 'these', \"that'll\", 'having', 'or', 'did', 'between', 'haven', 'when', 'can', 'this', 'd', \"couldn't\", 'and', 'all', 'few', 'the', 'against', \"hadn't\", 'my', 'any', 'own', 'your', 'which', 'has', 'most', 'if', 'now', 'at', 'his', \"don't\", 'do', 'him', 'mightn', 'those', 'she', 'herself', 'under', 'had', 'itself', 'whom', 'should', 'until', 'he', 'some', 'theirs', \"needn't\", 'aren', 'so', \"mightn't\", 'over', 'for', \"mustn't\", 're', \"shouldn't\", 'won', 'yours', 'is', 'before', 'how', 'weren', 'm', 'where', 'does', \"isn't\", 'further', 'i', 'll', 'shan', 'than', 'once', 'that', 'doing', 'it', 'am', 'hadn', 'from', 'again', 'same', 'hasn', 'himself', 'only', \"didn't\", 'each', \"hasn't\", 'because', 'by', 'been', 'we', \"it's\", 'more', 'to', 'couldn', 'be', 'o', 'there', 'them', 'their', 'no', 'below', 'on', 't', 'but', 'of', 'off', 'needn', 'have', 'didn', 'up', 'its', 'a', 'here', 'shouldn'}\n",
            "***********************************************************\n",
            "tasti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRX8mg4csgQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "str1 = ' '\n",
        "# Creating a list of final strings after all of the processings\n",
        "final_string = []\n",
        "all_positive_words = [] # store words from +ve reviews here, so as process them later.\n",
        "all_negative_words = [] # # store words from -ve reviews here, so as process them later.\n",
        "s = ''\n",
        "\n",
        "for sent in final['Text'].values: # for each sentence in final text value.\n",
        "  filtered_sentence=[]\n",
        "  sent = cleanhtml(sent) # clear all html tags.\n",
        "  for w in sent.split(): # splitting the sentence into words.\n",
        "    for cleaned_words in cleanpunc(w).split(): # clearing the punctuations from each words.\n",
        "      if((cleaned_words.isalpha()) & (len(cleaned_words)>2)): # checking if it alpha-numeric and length > 2.\n",
        "        if(cleaned_words.lower() not in stop): # converting it to lower case.\n",
        "           s = (sno.stem(cleaned_words.lower())).encode('utf8') # doing the stemming process.\n",
        "           filtered_sentence.append(s)  # storing the final words after passing it through all the conditions.\n",
        "           if(final['Score'].values)[i] == 'positive':\n",
        "             all_positive_words.append(s) # list of all positive words.\n",
        "           if(final['Score'].values)[i] == 'negative':\n",
        "             all_negative_words.append(s) # list of all negative words.\n",
        "        else:\n",
        "          continue\n",
        "      else:\n",
        "        continue\n",
        "  # Join the filtered sentences to construct the final sentence.\n",
        "  str1 = b\" \".join(filtered_sentence)  # final string of cleaned words.\n",
        "\n",
        "  final_string.append(str1)\n",
        "  i+=1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc8yuOd6su5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding another new column of CleanedText after doing all the processing.\n",
        "final['CleanedText'] = final_string\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgpFim67tHHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "354de1ac-c134-4bf6-e8cd-95d04da95035"
      },
      "source": [
        "final.head(3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>CleanedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>138706</th>\n",
              "      <td>150524</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>ACITT7DI6IDDL</td>\n",
              "      <td>shari zychinski</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>positive</td>\n",
              "      <td>939340800</td>\n",
              "      <td>EVERY book is educational</td>\n",
              "      <td>this witty little book makes my son laugh at l...</td>\n",
              "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138688</th>\n",
              "      <td>150506</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>A2IW4PEEKO2R0U</td>\n",
              "      <td>Tracy</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1194739200</td>\n",
              "      <td>Love the book, miss the hard cover version</td>\n",
              "      <td>I grew up reading these Sendak books, and watc...</td>\n",
              "      <td>b'grew read sendak book watch realli rosi movi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138689</th>\n",
              "      <td>150507</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>A1S4A3IQ2MU7V4</td>\n",
              "      <td>sally sue \"sally sue\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1191456000</td>\n",
              "      <td>chicken soup with rice months</td>\n",
              "      <td>This is a fun way for children to learn their ...</td>\n",
              "      <td>b'fun way children learn month year learn poem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Id  ...                                        CleanedText\n",
              "138706  150524  ...  b'witti littl book make son laugh loud recit c...\n",
              "138688  150506  ...  b'grew read sendak book watch realli rosi movi...\n",
              "138689  150507  ...  b'fun way children learn month year learn poem...\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJKwSY9uDpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing final table into a sqlite table for future, so that I dont need to keep repeating this preprocessing\n",
        "conn = sqlite3.connect('/content/drive/My Drive/amazon_fine_food/final.sqlite')\n",
        "c = conn.cursor()\n",
        "conn.text_factory = str\n",
        "final.to_sql('Reviews', conn, schema = None, if_exists = 'replace')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiZDwyPjxaRg",
        "colab_type": "text"
      },
      "source": [
        "##Bi-Grams and n-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtHvH-NxUhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bdfc38e1-9b2b-47f8-8307-1f82daea9d06"
      },
      "source": [
        "# Let's see which word occur more often in my positive review and negative reviews.\n",
        "freq_dist_positive = nltk.FreqDist(all_positive_words)\n",
        "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
        "print(\"Most Common Positive Words :\", freq_dist_positive.most_common(20))\n",
        "print(\"Most Common Negative Words :\", freq_dist_negative.most_common(20))\n",
        "\n",
        "# Output is printed as words with their frequency."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Common Positive Words : [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
            "Most Common Negative Words : [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzepv4Ucyh4b",
        "colab_type": "text"
      },
      "source": [
        "From above we see that mose common positive and negative words overlap. For eg: 'like' which could be used as 'not like' etc.\n",
        "So, its good to consider pairs of consequent words (bi-grams) or n grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtwzYwh9y4TT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3870653c-28ef-4868-c870-0859e1061f64"
      },
      "source": [
        "count_vect = CountVectorizer(ngram_range=(1,2))   # 1 --> unigram and 2 --> bigrams.\n",
        "\n",
        "# When I used Unigrams I had 115k dimensions and when I used bigrams, the number of dim increases to 2910K (as I have 1gram and 2gram both).\n",
        "final_bigram_counts = count_vect.fit_transform(final['Text'].values)\n",
        "final_bigram_counts.get_shape()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364171, 2910192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyeX1pJw0Xhb",
        "colab_type": "text"
      },
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxmnk8hl0bPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2)) # It also have ngram param.\n",
        "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)  # I just gave raw data here."
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRJoOb202_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cf85dff-222d-4d15-a6a9-95868fad633a"
      },
      "source": [
        "final_tf_idf.get_shape()  # Get exactly same as above 2910K."
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364171, 2910192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH96qJaz1Vwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bb09c57-e77e-4074-cf8d-17206c9e1b7a"
      },
      "source": [
        "# If I want to get each of the feature names (here, the list of words, as each word is dim here).\n",
        "features = tf_idf_vect.get_feature_names()\n",
        "len(features)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2910192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wHKREBO1rp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "5cf71a8d-2543-4bec-e482-4359aabc2066"
      },
      "source": [
        "# Lets try to print some 10 features here.\n",
        "features[100000:100010]\n",
        "\n",
        "# These are bigrams, unigrams..."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ales until',\n",
              " 'ales ve',\n",
              " 'ales would',\n",
              " 'ales you',\n",
              " 'alessandra',\n",
              " 'alessandra ambrosia',\n",
              " 'alessi',\n",
              " 'alessi added',\n",
              " 'alessi also',\n",
              " 'alessi and']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGT6HthX17Lf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "92fd1ef9-0df6-416c-c6b2-aac71fa99428"
      },
      "source": [
        "# Get the vector of review 3.\n",
        "# Convert a row in sparsematrix to a numpy array\n",
        "import numpy as np\n",
        "print(np.array(final_tf_idf[3,:]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2857710)\t0.12516284773073685\n",
            "  (0, 1468762)\t0.12516284773073685\n",
            "  (0, 2579212)\t0.07104099313724015\n",
            "  (0, 1034587)\t0.052718834273936616\n",
            "  (0, 2436412)\t0.08226308798444909\n",
            "  (0, 1034365)\t0.12516284773073685\n",
            "  (0, 458832)\t0.14613739881457247\n",
            "  (0, 1334019)\t0.11412869651927751\n",
            "  (0, 2818673)\t0.06679413440460479\n",
            "  (0, 1394881)\t0.07292728198961108\n",
            "  (0, 869883)\t0.07645300926395387\n",
            "  (0, 2859157)\t0.07942414973798338\n",
            "  (0, 2565801)\t0.07272882367484192\n",
            "  (0, 159362)\t0.03756535633266459\n",
            "  (0, 2598521)\t0.0509941884610428\n",
            "  (0, 1478400)\t0.05139544868418497\n",
            "  (0, 997429)\t0.058490230406077925\n",
            "  (0, 1183590)\t0.11467604291955785\n",
            "  (0, 2542781)\t0.09565729858861444\n",
            "  (0, 1276133)\t0.055596165637067714\n",
            "  (0, 2142988)\t0.10979995636542227\n",
            "  (0, 2650620)\t0.12516284773073685\n",
            "  (0, 2619109)\t0.07802654664202313\n",
            "  (0, 1239571)\t0.058975199933590856\n",
            "  (0, 1795444)\t0.08242442423807618\n",
            "  :\t:\n",
            "  (0, 991870)\t0.015558528071919465\n",
            "  (0, 2743883)\t0.04926122937961985\n",
            "  (0, 2502558)\t0.018127513404642612\n",
            "  (0, 2212820)\t0.10805960574428872\n",
            "  (0, 2578785)\t0.025907991501130486\n",
            "  (0, 105294)\t0.03853052694071944\n",
            "  (0, 1276123)\t0.0259101746440072\n",
            "  (0, 1464294)\t0.10658816026026013\n",
            "  (0, 1027762)\t0.050158050457773606\n",
            "  (0, 2836073)\t0.02592649124581482\n",
            "  (0, 2606992)\t0.06577304751596659\n",
            "  (0, 1317342)\t0.014244444064321692\n",
            "  (0, 1731751)\t0.029244575076570276\n",
            "  (0, 101342)\t0.024454795807582096\n",
            "  (0, 2142935)\t0.07942414973798338\n",
            "  (0, 49389)\t0.02843040846516642\n",
            "  (0, 139736)\t0.03489177783968005\n",
            "  (0, 204974)\t0.04334367230004227\n",
            "  (0, 2512439)\t0.06812091493423356\n",
            "  (0, 1266669)\t0.03169560655255301\n",
            "  (0, 1332766)\t0.040354591536011016\n",
            "  (0, 361959)\t0.12498820746031622\n",
            "  (0, 1463851)\t0.08986493508764523\n",
            "  (0, 2857708)\t0.10805960574428872\n",
            "  (0, 2574871)\t0.027502037583348224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--iPuGJ3yXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_tfidf_feats(row, features, top_n=25):  # getting top 25 tfidf\n",
        "  # Get top tfidf in a row and return them with corresponing ranks\\\n",
        "  # argsort is to sort.\n",
        "  topn_ids = np.argsort(row)[::-1][:top_n]\n",
        "  top_feats = [(features[i], row[i]) for i in topn_ids]\n",
        "  df = pd.DataFrame(top_feats)\n",
        "  df.columns = ['features', 'tfidf']\n",
        "  return df\n",
        "\n",
        "# for vector 1 corresponding to review 1, converting it to numpy array, giving all feature values, and getting top 25.\n",
        "top_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8YQwsMH48gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "outputId": "cf2f6095-5b58-481b-be3b-ecf304573b4e"
      },
      "source": [
        "top_tfidf\n",
        "# I am getting the top 25 terms in given review.\n",
        "# tfidf vector is also a sparse matrix."
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sendak books</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rosie movie</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>paperbacks seem</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cover version</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>these sendak</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the paperbacks</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>pages open</td>\n",
              "      <td>0.173437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>really rosie</td>\n",
              "      <td>0.168074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>incorporates them</td>\n",
              "      <td>0.168074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>paperbacks</td>\n",
              "      <td>0.168074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>however miss</td>\n",
              "      <td>0.164269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>hard cover</td>\n",
              "      <td>0.164269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>seem kind</td>\n",
              "      <td>0.161317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>up reading</td>\n",
              "      <td>0.156867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>that incorporates</td>\n",
              "      <td>0.155100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>the pages</td>\n",
              "      <td>0.149737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>sendak</td>\n",
              "      <td>0.149737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>rosie</td>\n",
              "      <td>0.146786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>of flimsy</td>\n",
              "      <td>0.146786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>two hands</td>\n",
              "      <td>0.145130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>movie that</td>\n",
              "      <td>0.144374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>reading these</td>\n",
              "      <td>0.137184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>too do</td>\n",
              "      <td>0.134491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>incorporates</td>\n",
              "      <td>0.134147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>flimsy and</td>\n",
              "      <td>0.132254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             features     tfidf\n",
              "0        sendak books  0.173437\n",
              "1         rosie movie  0.173437\n",
              "2     paperbacks seem  0.173437\n",
              "3       cover version  0.173437\n",
              "4        these sendak  0.173437\n",
              "5      the paperbacks  0.173437\n",
              "6          pages open  0.173437\n",
              "7        really rosie  0.168074\n",
              "8   incorporates them  0.168074\n",
              "9          paperbacks  0.168074\n",
              "10       however miss  0.164269\n",
              "11         hard cover  0.164269\n",
              "12          seem kind  0.161317\n",
              "13         up reading  0.156867\n",
              "14  that incorporates  0.155100\n",
              "15          the pages  0.149737\n",
              "16             sendak  0.149737\n",
              "17              rosie  0.146786\n",
              "18          of flimsy  0.146786\n",
              "19          two hands  0.145130\n",
              "20         movie that  0.144374\n",
              "21      reading these  0.137184\n",
              "22             too do  0.134491\n",
              "23       incorporates  0.134147\n",
              "24         flimsy and  0.132254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMwHoSg35yZO",
        "colab_type": "text"
      },
      "source": [
        "##Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhc2NdbX3yOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Google News W2V.\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhy5XZRH1iJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in this file, for every word there a vector, its been collected using google news."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnoBFA9-7dur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz')\n",
        "# This file contain 300D vectors."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd0rKyb17sW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv['computer']\n",
        "# To get the vector for a particular word."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCqgr1iq77GH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.similarity('woman','man')\n",
        "# Finding numeric similarity between words. It ranges for 0 to 1.\n",
        "# 1 means exactly same.\n",
        "# 0 means very very diff. It is the min value."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_LIKKES8J43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.vw.most_similar('woman')\n",
        "# Finding most similar words to woman. It gives output in decreasing order of similarities. Its case-sensitive. \n",
        "# Ex: 'woman' and 'Woman' may differ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MomjJhOz8yUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.wv.most_similar('tasti')\n",
        "# The above line will generate error as there is no word tasti. So we may get error in some cases of stem words.\n",
        "model.wv.most_similar('tasty')\n",
        "# This will give words similar to tasty.\n",
        "# Similarity values are large if points are closer."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEF_8onA9cNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.similarity('tasty','tast')\n",
        "# Finding similarity between two words."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l12M9KEU9xcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are creating the list of sentences consisting of words.\n",
        "import gensim\n",
        "i=0\n",
        "list_of_sent = []\n",
        "for sent in final['Text'].values:\n",
        "  filtered_sentence=[]\n",
        "  sent=cleanhtml(sent)\n",
        "  for w in sent.split():\n",
        "    for cleaned_words in cleanpunc(w).split():\n",
        "      if(cleaned_words.isalpha()):\n",
        "        filtered_sentence.append(cleaned_words.lower())\n",
        "      else:\n",
        "        continue\n",
        "  list_of_sent.append(filtered_sentence)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlnBFa4bV_DT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cba0e8a0-77ac-4b20-d778-bd2276a80dcb"
      },
      "source": [
        "print(final['Text'].values[0])\n",
        "print(\"***********************************\")\n",
        "print(list_of_sent[0])\n",
        "\n",
        "# We that the particular printed sentence is converted into the list."
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
            "***********************************\n",
            "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'were', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'hes', 'learned', 'about', 'whales', 'india', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'still', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEt2gUhBWi81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model = gensim.models.Word2Vec(list_of_sent,min_count=5,size=50,workers=4)\n",
        "# min_count means if the words does not occur atleast 5 times then dont construct w2v for it.\n",
        "# size means what dimensional vector do you want of the word."
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u84Kag-WYuDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "766eee61-4407-4233-fed5-e8d1ba58c694"
      },
      "source": [
        "words = list(w2v_model.wv.vocab)\n",
        "print(len(words))\n",
        "# I can get dict of all the words I have."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "957UZQhYZPWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ddb4d9e7-4831-4d91-ef9d-b22f013dde0d"
      },
      "source": [
        "w2v_model.wv.most_similar('tasty')\n",
        "# Finding similar words for tasty in our corpus (review dataset)."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tastey', 0.9078287482261658),\n",
              " ('satisfying', 0.8583592176437378),\n",
              " ('yummy', 0.8533511161804199),\n",
              " ('delicious', 0.8193768262863159),\n",
              " ('filling', 0.8150377869606018),\n",
              " ('flavorful', 0.7987992763519287),\n",
              " ('nutritious', 0.7558767199516296),\n",
              " ('addicting', 0.7549490332603455),\n",
              " ('delish', 0.7477218508720398),\n",
              " ('delectable', 0.7432020902633667)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDNr8FUaMv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "481ae4bc-a585-4f55-d623-bb2b3c3de656"
      },
      "source": [
        "w2v_model.wv.most_similar('like')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('resemble', 0.7128250002861023),\n",
              " ('dislike', 0.6608422994613647),\n",
              " ('mean', 0.6542459726333618),\n",
              " ('prefer', 0.6403476595878601),\n",
              " ('overpower', 0.6094322204589844),\n",
              " ('think', 0.6085252165794373),\n",
              " ('enjoy', 0.5944650173187256),\n",
              " ('miss', 0.5902699828147888),\n",
              " ('overwhelm', 0.5893973708152771),\n",
              " ('expect', 0.5830036401748657)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TXdVXFaVi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6860cdb9-e06d-4eae-b8b4-438efb14a00d"
      },
      "source": [
        "count_vect_feat = count_vect.get_feature_names()  # list of words.\n",
        "print(count_vect_feat.index('like'))\n",
        "print(count_vect_feat[1442686])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1442686\n",
            "like\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0JCU7rYbKle",
        "colab_type": "text"
      },
      "source": [
        "##Avg W2V and TFIDF W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytx50IJTbPE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "34847275-cb4d-47b3-871a-2e094cd751ef"
      },
      "source": [
        "# computing avf w2v for each review.\n",
        "sent_vectors = [];  # avg-w2v for each sentence is stored in this\n",
        "for sent in list_of_sent: # for each review\n",
        "  sent_vec = np.zeros(50) # as word vectors are of zero length\n",
        "  cnt_words = 0; # num of words with the valid vec in the sentence\n",
        "  for words in sent:  # for each word in review\n",
        "    try:\n",
        "      vec = w2v_model.wv[word]   # computing w2v\n",
        "      sent_vec += vec  # adding each to sentence vec\n",
        "      con_words += 1\n",
        "    except:\n",
        "      pass\n",
        "  sent_vec /= cnt_words  # divide it by no. of words\n",
        "  sent_vectors.append(sent_vec)\n",
        "print(len(sent_vectors))\n",
        "print(len(sent_vectors[0]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "364171\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOYGSLHCq8Nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61cabc99-94b7-4d59-d665-242541dd86d5"
      },
      "source": [
        "tfidf_feat = tf_idf_vect.get_feature_names()  # tfidf words/col names\n",
        "# final_tf_idf is the sparse matrix with row=sentence, col=word and cell_value.\n",
        "\n",
        "tfidf_sent_vectors = [];  # the tfidf-w2v for each review is stored here.\n",
        "row = 0;\n",
        "for sent in list_of_sent:  # for each review\n",
        "  sent_vec = np.zeros(50)  # as word vec are of zero length\n",
        "  weight_sum = 0;  # num of words with a valid vec in the review\n",
        "  for word in sent:\n",
        "    try:\n",
        "      vec = w2v_model.wv[word]\n",
        "      # obtain the tfidf of a word in a sentence\n",
        "      tfidf = final_tf_idf[row,tfidf_feat.index(word)]\n",
        "      sent_vec += (vec * tf_idf)\n",
        "      weight_sum += tf_idf\n",
        "    except:\n",
        "      pass\n",
        "  sent_vec /= weight_sum\n",
        "  tfidf_sent_vectors.append(sent_vec)\n",
        "  row += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}